{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib. request\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping all rental data \n",
    "\n",
    "# Scraping entire data may take over 18 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_immobili(type_house,max_pages):\n",
    "    for seite in range(1,max_pages):\n",
    "    \n",
    "        print(\"Loop \" + str(seite) + \" startet.\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        l=[]\n",
    "\n",
    "        try:\n",
    "\n",
    "            soup = bs.BeautifulSoup(urllib.request.urlopen(\"https://www.immobilienscout24.de/Suche/S-2/P-\"+str(seite)+\"/\" + type_house).read(),'lxml')\n",
    "            print(\"Aktuelle Seite: \"+\"https://www.immobilienscout24.de/Suche/S-2/P-\"+str(seite)+\"/\"+type_house)\n",
    "            for paragraph in soup.find_all(\"a\"):\n",
    "\n",
    "                if r\"/expose/\" in str(paragraph.get(\"href\")):\n",
    "                    l.append(paragraph.get(\"href\").split(\"#\")[0])\n",
    "\n",
    "                l = list(set(l))\n",
    "\n",
    "            for item in l:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    soup = bs.BeautifulSoup(urllib.request.urlopen('https://www.immobilienscout24.de'+item).read(),'lxml')\n",
    "\n",
    "                    data = pd.DataFrame(json.loads(str(soup.find_all(\"script\")).split(\"keyValues = \")[1].split(\"}\")[0]+str(\"}\")),index=[str(datetime.now())])\n",
    "\n",
    "                    data[\"URL\"] = str(item)\n",
    "\n",
    "                    beschreibung = []\n",
    "\n",
    "                    for i in soup.find_all(\"pre\"):\n",
    "                        beschreibung.append(i.text)\n",
    "\n",
    "                    data[\"beschreibung\"] = str(beschreibung)\n",
    "\n",
    "                    df = df.append(data)\n",
    "\n",
    "                except Exception as e: \n",
    "                    print(str(datetime.now())+\": \" + str(e))\n",
    "                    l = list(filter(lambda x: x != item, l))\n",
    "                    print(\"ID \" + str(item) + \" entfernt.\")\n",
    "            print(\"Exportiert CSV\")\n",
    "            df.to_csv(str(datetime.now())[:19].replace(\":\",\"\").replace(\".\",\"\")+\".csv\",sep=\";\",decimal=\",\",encoding = \"utf-8\",index_label=\"timestamp\")     \n",
    "\n",
    "            print(\"Loop \" + str(seite) + \" endet.\\n\")\n",
    "\n",
    "        except Exception as e: \n",
    "            print(str(datetime.now())+\": \" + str(e))\n",
    "\n",
    "    print(\"End of scraping!\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    n = 0\n",
    "    for i in os.listdir()[2:-2]:\n",
    "        n = n + 1\n",
    "        df = df.append(pd.read_csv(str(i),sep = \";\" , decimal = \",\" , encoding = \"utf-8\" ))\n",
    "        print ( \"passage\" + str ( n ) )\n",
    "    \n",
    "    df.to_csv(type_houseouse + '_data.csv')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping 4250 pages of wohnung-mieten (rent)\n",
    "1. Make sure that entire directory is empty before running the scraping function every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_immobili('wohnung-mieten',4250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clear the dirctory before running the next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_immobili('haus-mieten',230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clear the dirctory before running the next function\n",
    "# Scraping Purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_immobili('wohnung-kaufen',1650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clear the dirctory before running the next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_immobili('haus-kaufen',2300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note for Professor\n",
    "1. since entire scraping would take over 18hrs we decided to scrape different pages saperately and merge the data together at the end to save time\n",
    "\n",
    "2. The number of pages that was available to scrape was initially estimated roughly based on the number of results that was shown, later we did a binary search to find exact number of pages, this wasen't done using any code so we haven't included it. Approximation used ~ (No of search results)/20 (as every page shows only 20 results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
