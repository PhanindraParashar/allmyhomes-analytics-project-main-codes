{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "from pycaret.classification import *\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold,KFold, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler,RobustScaler \n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression,Lasso\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score,mean_squared_error,classification_report,confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "import pycaret\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import logit, probit, poisson, ols\n",
    "import statsmodels.formula.api as smf\n",
    "from pycaret.regression import *\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "lin_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "def lr_pred(df,target):\n",
    "    cols = [c for c in df.columns if c!= target]\n",
    "    X = df[cols]\n",
    "    y = df[target].values\n",
    "    \n",
    "    model = lin_model.fit(X, y)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    \n",
    "    r2 = r2_score(y,y_pred)\n",
    "    \n",
    "    dr = df.copy()\n",
    "    dr['pred'] = y_pred\n",
    "    \n",
    "    return dr\n",
    "    \n",
    "\n",
    "def lr(df,target,cols):\n",
    "    X = df[cols]\n",
    "    y = df[target].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=0)\n",
    "    model = lin_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "    \n",
    "    return r2\n",
    "def num_col(dmodel,type_of,rest):\n",
    "    df = dmodel.copy()\n",
    "    num = []\n",
    "    for i in range(len((dmodel.dtypes == type_of).values)):\n",
    "        v = (dmodel.dtypes == type_of)\n",
    "        if v.values[i]:\n",
    "            if v.index[i] not in rest:\n",
    "                num.append(v.index[i])\n",
    "    return num\n",
    "\n",
    "\n",
    "\n",
    "def col_to_dobble_log(dmodel,type_of,rest,target):\n",
    "    dr = col_to_log(dmodel,type_of,rest,target)\n",
    "    df = dmodel.copy()\n",
    "    \n",
    "    num = num_col(dmodel,type_of,rest)\n",
    "    \n",
    "    df_log_log = np.log(np.log(df[num]+.0001)+0.0001)\n",
    "    \n",
    "    col = df_log_log.columns\n",
    "    ncol = []\n",
    "    for i in col:\n",
    "        ncol.append(i+'_log_log')\n",
    "    \n",
    "    df_log_log.columns = ncol\n",
    "    \n",
    "    for i in df_log_log.columns:\n",
    "        dr[i] = df_log_log[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dll = dr.copy()\n",
    "    \n",
    "    cols_nan = []\n",
    "    for i in range(len(np.isfinite(dll).sum() != len(dll))):\n",
    "        v = (np.isfinite(dll).sum() != len(dll))[i]\n",
    "        if v:\n",
    "            cols_nan.append(i)\n",
    "    Cols = []\n",
    "    for i in range(len(dll.columns)):\n",
    "        if i not in cols_nan:\n",
    "            Cols.append(dll.columns[i])\n",
    "    \n",
    "    dll = dll[Cols]\n",
    "    \n",
    "    \n",
    "    t1 = target + '_log'\n",
    "    t2 = target + '_log_log'\n",
    "    \n",
    "    #print(t2,t2 in dll.columns)\n",
    "    \n",
    "    if t1 in dll.columns:\n",
    "        #print('.....')\n",
    "        if t2 in dll.columns:\n",
    "            dll.drop([t1,t2],axis=1,inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dll\n",
    "def i_format(i):\n",
    "    #print(i)\n",
    "    if '_log_log' in i:\n",
    "        return i[:i.index('_log_log')],i[:i.index('_log_log')+4],i\n",
    "    \n",
    "    else:\n",
    "        if '_log' in i:\n",
    "            return i[:i.index('_log')],i,i+'_log'\n",
    "        \n",
    "        if '_log' not in i:\n",
    "            return i,i+'_log',i+'_log_log'\n",
    "\n",
    "def forbiden_col(target,column_present):\n",
    "    forbiden = [target]\n",
    "    for i in column_present:\n",
    "        #print(i)\n",
    "        a,b,c = i_format(i)\n",
    "        forbiden.append(a)\n",
    "        forbiden.append(b)\n",
    "        forbiden.append(c)\n",
    "    return forbiden\n",
    "\n",
    "def col_add(df,col_name,data):\n",
    "    dfr = df.copy()\n",
    "    dfr[col_name] = data\n",
    "    return dfr\n",
    "\n",
    "def best_farward_prop(df,target,column_present,p_r2):\n",
    "    dr = df[column_present].copy()\n",
    "    #print(dr.head())\n",
    "    if len(column_present) < 3 :\n",
    "        dr['dummy1'] = 1\n",
    "        dr['dummy2'] = 2\n",
    "        \n",
    "    forbiden = forbiden_col(target,column_present)\n",
    "    \n",
    "\n",
    "    \n",
    "    col = [c for c in df.columns if c not in forbiden]\n",
    "    \n",
    "    features = col.copy()\n",
    "    \n",
    "   \n",
    "    Fwd = []\n",
    "    R2 = []\n",
    "    \n",
    "    \n",
    "    for i in features:\n",
    "        dfr = col_add(dr,i,df[i])\n",
    "        dfr_y = col_add(dfr,target,df[target])\n",
    "        #return dfr_y,target\n",
    "        #print(dfr_y.head())\n",
    "        r2 = lr(dfr_y,target,[c for c in dfr_y.columns if c!= target])\n",
    "        #print(r2)\n",
    "        \n",
    "        if r2 > p_r2:\n",
    "            R2.append(r2)\n",
    "            Fwd.append(i)\n",
    "            #print(r2,i)\n",
    "    \n",
    "    if len(R2) > 0:\n",
    "        #print(max(R2),Fwd[R2.index(max(R2))])\n",
    "        return max(R2),Fwd[R2.index(max(R2))]\n",
    "    \n",
    "    else:\n",
    "        return p_r2,''\n",
    "\n",
    "def forward_prop(df,target,type_of,rest):\n",
    "    \n",
    "    dr = df.copy()\n",
    "    column_present = [best_farward_prop(dr,target,[],0)[1]]\n",
    "    \n",
    "    forbiden = forbiden_col(target,column_present)\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_pred = np.ones(len(df))*np.mean(df[target].values)\n",
    "    \n",
    "    best_r2 = r2_score(y_pred,df[target].values)\n",
    "    print('initial r2 is ',best_r2)\n",
    "    \n",
    "    for i in dr.columns:\n",
    "        if i not in forbiden:\n",
    "            #print(column_present)\n",
    "            p_r2,feature = best_farward_prop(dr,target,column_present,best_r2)\n",
    "            if p_r2 > best_r2:\n",
    "                best_r2 = p_r2\n",
    "                column_present.append(feature)\n",
    "                print(best_r2,feature)\n",
    "            else:\n",
    "                break\n",
    "    return best_r2,column_present\n",
    "        \n",
    "        \n",
    "\n",
    "def model_fwd(df,target,type_of,rest):\n",
    "    dc_log_log = col_to_dobble_log(df,type_of,rest,target)\n",
    "    return forward_prop(dc_log_log,target,type_of,rest)\n",
    "\n",
    "def add_catef(df,dfcat):\n",
    "    dr = df.copy()\n",
    "    for i in dfcat.columns:\n",
    "        dr[i] = dfcat[i]\n",
    "    return dr\n",
    "\n",
    "def features_to_drop_backward(df,target,rest):\n",
    "    col = [c for c in df.columns if c != target]\n",
    "    \n",
    "    main_model_r2 = lr(df,target,col)\n",
    "    best_r2 = main_model_r2\n",
    "    \n",
    "    features = col.copy()\n",
    "    \n",
    "    Drp = []\n",
    "    R2 = []\n",
    "    \n",
    "    for i in features:\n",
    "        if i not in rest:\n",
    "            d_col = [c for c in features if c!= i]\n",
    "            d_r2 = lr(df,target,d_col)\n",
    "\n",
    "            if d_r2 > best_r2:\n",
    "                Drp.append(i)\n",
    "                R2.append(d_r2)\n",
    "                #print(d_r2,i)\n",
    "    \n",
    "    if len(Drp) > 0:\n",
    "        best = max(R2)\n",
    "        droped = Drp[R2.index(best)]\n",
    "        return best,droped\n",
    "    else:\n",
    "        return best_r2,''\n",
    "\n",
    "def step_back(df,target,rest):\n",
    "    \n",
    "    dr = df.copy()\n",
    "    \n",
    "    col = [c for c in df.columns if c != target]\n",
    "    main_model_r2 = lr(df,target,col)\n",
    "    \n",
    "    best_all = main_model_r2\n",
    "    \n",
    "    features = col.copy()\n",
    "    \n",
    "    for j in range(len(col)-len(rest)):\n",
    "        r2,droped = features_to_drop_backward(dr,target,rest)\n",
    "        if droped != '':\n",
    "            print('droped column ',droped)\n",
    "            dr.drop(droped,axis=1,inplace=True)\n",
    "        if droped == '':\n",
    "            break\n",
    "    return best_all,r2,dr.columns\n",
    "\n",
    "def step_backward_with_log(dmodel,target,type_of,rest):\n",
    "    dll = col_to_log(dmodel,type_of,rest,target)\n",
    "    \n",
    "    t1 = target + '_log'\n",
    "    \n",
    "    if t1 in dll.columns:\n",
    "        dll.drop([t1],axis=1,inplace = True)\n",
    "    \n",
    "    k = len(dll)\n",
    "\n",
    "    cols_nan = []\n",
    "    for i in range(len(np.isfinite(dll).sum() != k)):\n",
    "        v = (np.isfinite(dll).sum() != k)[i]\n",
    "        if v:\n",
    "            cols_nan.append(i)\n",
    "\n",
    "    Cols = []\n",
    "    for i in range(len(dll.columns)):\n",
    "        if i not in cols_nan:\n",
    "            Cols.append(dll.columns[i])\n",
    "    \n",
    "    dll = dll[Cols]\n",
    "    \n",
    "    \n",
    "    return step_back(dll,target,rest)\n",
    "\n",
    "def step_backward_with_dubble_log(dmodel,target,type_of,rest):\n",
    "    dll = col_to_dobble_log(dmodel,type_of,rest,target)\n",
    "    \n",
    "    t1 = target + '_log'\n",
    "    t2 = target + '_log_log'\n",
    "    \n",
    "    if t1 in dll.columns:\n",
    "        if t2 in dll.columns:\n",
    "            dll.drop([t1,t2],axis=1,inplace = True)\n",
    "\n",
    "    k = len(dll)\n",
    "    \n",
    "    cols_nan = []\n",
    "    for i in range(len(np.isfinite(dll).sum() != k)):\n",
    "        v = (np.isfinite(dll).sum() != k)[i]\n",
    "        if v:\n",
    "            cols_nan.append(i)\n",
    "\n",
    "    Cols = []\n",
    "    for i in range(len(dll.columns)):\n",
    "        if i not in cols_nan:\n",
    "            Cols.append(dll.columns[i])\n",
    "    \n",
    "    dll = dll[Cols]\n",
    "    \n",
    "    return step_back(dll,target,rest)\n",
    "\n",
    "\n",
    "def step_backward_only(dll,target,type_of,rest):\n",
    "    #dll = col_to_dobble_log(dmodel,type_of,rest,target)\n",
    "    \n",
    "    t1 = target + '_log'\n",
    "    t2 = target + '_log_log'\n",
    "    \n",
    "    if t1 in dll.columns:\n",
    "        if t2 in dll.columns:\n",
    "            dll.drop([t1,t2],axis=1,inplace = True)\n",
    "\n",
    "    k = len(dll)\n",
    "    \n",
    "    cols_nan = []\n",
    "    for i in range(len(np.isfinite(dll).sum() != k)):\n",
    "        v = (np.isfinite(dll).sum() != k)[i]\n",
    "        if v:\n",
    "            cols_nan.append(i)\n",
    "\n",
    "    Cols = []\n",
    "    for i in range(len(dll.columns)):\n",
    "        if i not in cols_nan:\n",
    "            Cols.append(dll.columns[i])\n",
    "    \n",
    "    dll = dll[Cols]\n",
    "    \n",
    "    return step_back(dll,target,rest)\n",
    "\n",
    "def col_to_log(dmodel,type_of,rest,target):\n",
    "    df = dmodel.copy()\n",
    "    #df = df + 0.001\n",
    "    \n",
    "    num = num_col(dmodel,type_of,rest)\n",
    "    \n",
    "    df_log = np.log(df[num])\n",
    "    \n",
    "    col = df_log.columns\n",
    "    ncol = []\n",
    "    for i in col:\n",
    "        ncol.append(i+'_log')\n",
    "    \n",
    "    df_log.columns = ncol\n",
    "    \n",
    "    for i in df_log.columns:\n",
    "        df[i] = df_log[i]\n",
    "        \n",
    "    \n",
    "    dll = df.copy()\n",
    "    cols_nan = []\n",
    "    for i in range(len(np.isfinite(dll).sum() != len(dll))):\n",
    "        v = (np.isfinite(dll).sum() != len(dll))[i]\n",
    "        if v:\n",
    "            cols_nan.append(i)\n",
    "    Cols = []\n",
    "    for i in range(len(dll.columns)):\n",
    "        if i not in cols_nan:\n",
    "            Cols.append(dll.columns[i])\n",
    "    \n",
    "    dll = dll[Cols]\n",
    "    \n",
    "    t1 = target + '_log'\n",
    "    \n",
    "    if t1 in dll.columns:\n",
    "        dll.drop([t1],axis=1,inplace = True)\n",
    "    \n",
    "    \n",
    "    #dlog = pd.merge(df,df_log)\n",
    "    return dll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "def dist (la1,ln1,la2,ln2):\n",
    "    return geopy.distance.vincenty((la1,ln1), (la2,ln2)).km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Macro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_pop = pd.read_csv(r'C:\\Users\\phani\\Python Programming Codes Folder\\Analytics Project\\All_Macro_Data_Model\\Rental_macro_merge_inc_osm\\datasets_681859_1197071_plz_einwohner.csv')\n",
    "osm = pd.read_csv(r'C:\\Users\\phani\\Python Programming Codes Folder\\Analytics Project\\All_Macro_Data_Model\\Rental_macro_merge_inc_osm\\osm_data_macro_zip.csv')\n",
    "macro = pd.read_csv(r'C:\\Users\\phani\\Python Programming Codes Folder\\Analytics Project\\All_Macro_Data_Model\\Rental_macro_merge_inc_osm\\macro_zip_from_price_data_for_rent.csv')\n",
    "\n",
    "macro = macro[['zip', 'DG', 'A_total_of_1_room', '1_or_2_rooms', '3_rooms', '4_rooms',\n",
    "       '5_or_more_rooms', 'Total_population_of_age_18_25',\n",
    "       'Total_populatio__of_age_25_30', 'Total_population__of_age_30_40',\n",
    "       'Total_population_of_age_40_50', 'Deutsche', 'Foreigners',\n",
    "       'Wage_and_income_taxpayers', 'Total_amount_of_income',\n",
    "       'Wage_and_income_tax', 'gross_domestic_product',\n",
    "       'Gross_domestic_product_per_person_in_employment',\n",
    "       'Gross_domestic_product_per_inhabitant', 'amenity', 'bus',\n",
    "       'public_transport', 'cafe', 'doctors', 'fast_food', 'hospital',\n",
    "       'restaurant', 'zip_population', 'Population_State',\n",
    "       'GDP_Per_Capita_state', 'Area_State', 'log_price', 'Lat_reg2',\n",
    "       'Lng_reg2', 'Lat', 'Lng', 'lat_c', 'lng_c']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting all the data to of rent and purchase saperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('jun20_price_wohn_1_850.csv')\n",
    "df2 = pd.read_csv('jun20_price_wohn_850_1690.csv')\n",
    "frames = [df1,df2]\n",
    "dp_whn = pd.concat(frames)\n",
    "\n",
    "dp_whn.drop(['Unnamed: 0','timestamp'],axis=1,inplace=True)\n",
    "dp_whn = dp_whn.drop_duplicates()\n",
    "\n",
    "dr_whn = pd.read_csv(r'C:\\Users\\phani\\Python Programming Codes Folder\\Analytics Project\\Apr20 all data\\apr20_rental_no_duplicates.csv')\n",
    "dr_whn.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "dr_whn = dr_whn.drop_duplicates()\n",
    "\n",
    "dr_hau = pd.read_csv('jun20_rent_haus.csv')\n",
    "dr_hau.drop(['Unnamed: 0','timestamp'],axis=1,inplace=True)\n",
    "dr_hau = dr_hau.drop_duplicates()\n",
    "\n",
    "dp_hau = pd.read_csv(r'C:\\Users\\phani\\Python Programming Codes Folder\\Analytics Project\\Apr20 all data\\apr20_price.csv')\n",
    "dp_hau.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "dp_hau = dp_hau.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing irrelevant columns (Price to Rent Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_hau.drop(['obj_courtage',\n",
    " 'obj_purchasePriceRange',\n",
    " 'obj_constructionPhase',\n",
    " 'obj_rented',\n",
    " 'obj_international',\n",
    " 'obj_usableArea',\n",
    " 'obj_project_id',\n",
    " 'obj_nbp',\n",
    " 'Unnamed: 0.1'],axis=1,inplace=True)\n",
    "\n",
    "dr_hau.drop(['obj_serviceCharge',\n",
    "\n",
    " 'obj_hasKitchen',\n",
    " \n",
    " 'obj_petsAllowed',\n",
    " 'obj_depositLink',\n",
    " 'obj_baseRentRange',\n",
    " 'obj_assistedLiving',\n",
    " 'obj_heatingCosts'],axis=1,inplace=True)\n",
    "\n",
    "col_drop_r_hau = ['obj_telekomTvOffer', 'obj_cId','geo_bln',\n",
    "       'obj_newlyConst','obj_picture',\n",
    "       'obj_picturecount', 'obj_pricetrend', 'obj_telekomUploadSpeed','obj_telekomTrackingGroup',\n",
    "       'obj_telekomInternetTechnology','obj_telekomInternetType', 'obj_pricetrendbuy', 'obj_scoutId',\n",
    "       'obj_firingTypes','obj_telekomInternetProductName','obj_yearConstructedRange', 'obj_baseRent', 'obj_houseNumber',\n",
    "       'obj_pricetrendrent','geo_krs','ga_cd_cxp_historicallisting',\n",
    "       'obj_telekomDownloadSpeed', 'obj_street',\n",
    "       'obj_telekomInternetUrlMobile', 'obj_telekomInternetUrl',\n",
    "       'obj_streetPlain', 'geo_plz','obj_telekomHdTelephone', 'geo_land', 'ga_cd_via',\n",
    "       'obj_telekomInternet', 'obj_immotype', 'obj_telekomInternetServices',\n",
    "       'obj_telekomInternetProductAvailable', 'obj_cwId',\n",
    "       'ga_cd_application_requirements','obj_noRoomsRange', 'ga_cd_maillead_default_shown', 'obj_objectnumber',\n",
    "       'obj_livingSpaceRange','URL', 'beschreibung','obj_telekomHybridUploadSpeed', 'obj_groupnumber',\n",
    "       'obj_telekomHybridDownloadSpeed', 'evt_count_pm_sig','ga_cd_developer_virtualreality', 'obj_galleryAd',\n",
    "       'obj_energyType', 'obj_energyEfficiencyClass', 'obj_thermalChar',\n",
    "       'ga_cd_customer_group', 'ga_cd_via_qualified','obj_ityp']\n",
    "\n",
    "dr_hau.drop(col_drop_r_hau,axis=1,inplace=True)\n",
    "\n",
    "c = []\n",
    "for i in col_drop_r_hau:\n",
    "    if i in dp_hau.columns:\n",
    "        c.append(i)\n",
    "dp_hau.drop(c,axis=1,inplace=True)\n",
    "\n",
    "dp_hau.columns = ['obj_regio1', 'obj_heatingType', 'obj_plotAreaRange', 'obj_lotArea',\n",
    "       'obj_yearConstructed', 'obj_noParkSpaces', 'obj_ExclusiveExpose',\n",
    "       'obj_cellar', 'obj_livingSpace', 'zip', 'obj_condition',\n",
    "       'obj_interiorQual', 'obj_noRooms', 'obj_purchasePrice',\n",
    "       'obj_buildingType', 'obj_barrierFree', 'obj_regio3', 'obj_regio2',\n",
    "       'obj_lastRefurbish', 'obj_numberOfFloors']\n",
    "\n",
    "dr_hau.columns = ['obj_regio1', 'obj_heatingType', 'obj_plotAreaRange', 'obj_lotArea',\n",
    "       'obj_totalRent', 'obj_yearConstructed', 'obj_ExclusiveExpose',\n",
    "       'obj_cellar', 'obj_livingSpace', 'zip', 'obj_condition',\n",
    "       'obj_interiorQual', 'obj_noRooms', 'obj_numberOfFloors',\n",
    "       'obj_buildingType', 'obj_barrierFree', 'obj_regio3', 'obj_regio2',\n",
    "       'obj_noParkSpaces', 'obj_lastRefurbish']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Removal of outliers - as many houses have rent as 0 and living space as 0\n",
    "\n",
    "1. Living Space minimum was chosen as 20 (based on German Laws and Data Visualization - as they were mostly outliers)\n",
    "2. Similarly removing outliers based on Visualizations done via Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_house = dp_hau.copy()\n",
    "rent_house = dr_hau.copy()\n",
    "\n",
    "rent_house.drop(['obj_plotAreaRange'],axis=1,inplace=True)\n",
    "price_house.drop(['obj_plotAreaRange'],axis=1,inplace=True)\n",
    "\n",
    "price_house = price_house[price_house['obj_livingSpace'] > 20]\n",
    "price_house = price_house[price_house['obj_livingSpace'] < 700]\n",
    "price_house = price_house[price_house['obj_noRooms'] < 18]\n",
    "price_house = price_house[price_house['obj_lotArea'] < 3000]\n",
    "\n",
    "rent_house = rent_house[rent_house['obj_livingSpace'] > 20]\n",
    "rent_house = rent_house[rent_house['obj_livingSpace'] < 700]\n",
    "rent_house = rent_house[rent_house['obj_noRooms'] < 18]\n",
    "rent_house = rent_house[rent_house['obj_lotArea'] < 3000]\n",
    "\n",
    "rent_house = rent_house[rent_house['obj_totalRent'] > 100]\n",
    "rent_house = rent_house[rent_house['obj_totalRent'] < 12000]\n",
    "\n",
    "price_house = price_house[price_house['obj_purchasePrice'] > 60000]\n",
    "price_house = price_house[price_house['obj_purchasePrice'] < 10000000]\n",
    "\n",
    "price_house.drop(['obj_lastRefurbish','obj_numberOfFloors','obj_heatingType'],axis=1,inplace=True)\n",
    "rent_house.drop(['obj_lastRefurbish','obj_numberOfFloors','obj_heatingType'],axis=1,inplace=True)\n",
    "\n",
    "price_house.drop(['obj_noParkSpaces'],axis=1,inplace=True)\n",
    "rent_house.drop(['obj_noParkSpaces'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Helper functions to make pivot-tables\n",
    "1. Pivot table created using median price and median rent (As price to rent ratio is defined on Median) - Source: Investopedia\n",
    "2. since most of the macro features are either on a zip level or on a district level, and the data remains same in all the houses in a zip code, hence taking mean or median still remains the same.\n",
    "3. For micro features the mean and median makes a difference, however mean and median is almost the same and there wasn't any significant change in the outputs of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_basic_p_r(by,rent_house_macro,price_house_macro):\n",
    "    d1 = pd.pivot_table(rent_house_macro, index=[by,'obj_buildingType'],aggfunc=np.median).reset_index(level=['obj_buildingType'])\n",
    "    d2 = pd.pivot_table(price_house_macro, index=[by,'obj_buildingType'],aggfunc=np.median).reset_index(level=['obj_buildingType'])\n",
    "    d3 = d1.merge(d2,on=by)\n",
    "    d3 = d3[d3['obj_buildingType_x'] == d3['obj_buildingType_y']]\n",
    "    \n",
    "    ex = ['price_m2', 'anual_rent_m2']\n",
    "    cols = ['obj_buildingType', 'Area_State', 'DG', 'Deutsche', 'Foreigners', 'GDP_Per_Capita_state', 'Gross_domestic_product_per_inhabitant', 'Gross_domestic_product_per_person_in_employment', 'Lat', 'Lat_reg2', 'Lng', 'Lng_reg2', 'Population_State', 'Total_amount_of_income', 'Total_populatio__of_age_25_30', 'Total_population__of_age_30_40', 'Total_population_of_age_18_25', 'Total_population_of_age_40_50', 'Wage_and_income_tax', 'Wage_and_income_taxpayers', 'gross_domestic_product', 'public_transport', 'restaurant', 'zip_population']\n",
    "    #return d3\n",
    "    \n",
    "    c = []\n",
    "    for i in d3.columns:\n",
    "        if '_x' in i:\n",
    "            i = i.replace('_x','')\n",
    "        c.append(i)\n",
    "    d3.columns = c\n",
    "    \n",
    "    for i in d3.columns:\n",
    "        if '_y' in i:\n",
    "            d3.drop(i,axis=1,inplace=True)\n",
    "            \n",
    "    de = ['log_price','1_or_2_rooms', '3_rooms', '4_rooms',\n",
    "       '5_or_more_rooms', 'A_total_of_1_room',\n",
    "       'obj_livingSpace', 'obj_lotArea', 'obj_noRooms', 'obj_totalRent','obj_purchasePrice','a_r']\n",
    "    \n",
    "    for i in de:\n",
    "        if i in d3.columns:\n",
    "            d3.drop(i,axis=1,inplace=True)\n",
    "            \n",
    "    d3['Price_Rent_ratio'] = d3['price_m2']/d3['anual_rent_m2']\n",
    "    d3.drop(['price_m2','anual_rent_m2'],axis=1,inplace=True)\n",
    "    \n",
    "    #print(d3.columns)\n",
    "    \n",
    "    dc = d3.copy()\n",
    "\n",
    "    dc['tot_population'] = dc['Deutsche'] + dc['Foreigners']\n",
    "    dc['Foreigners_percent'] = 100*dc['Foreigners']/dc['tot_population']\n",
    "    dc.drop(['Foreigners','Deutsche'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    change_col = ['Total_populatio__of_age_25_30', 'Total_population__of_age_30_40',\n",
    "           'Total_population_of_age_18_25', 'Total_population_of_age_40_50']\n",
    "    dc['all_age_pop'] = dc[change_col[0]] + dc[change_col[1]] + dc[change_col[2]] + dc[change_col[3]]\n",
    "    dc['percent_pop_25_30'] = 100*(dc[change_col[0]]/dc['all_age_pop'])\n",
    "    dc['percent_pop_18_25'] = 100*(dc[change_col[2]]/dc['all_age_pop'])\n",
    "    dc['percent_pop_30_40'] = 100*(dc[change_col[1]]/dc['all_age_pop'])\n",
    "    dc['percent_pop_40_50'] = 100*(dc[change_col[3]]/dc['all_age_pop'])\n",
    "\n",
    "    dc.drop(change_col,axis=1,inplace=True)\n",
    "\n",
    "    dc['population_density_state'] = dc['Population_State']/dc['Area_State']\n",
    "    dc.drop(['Area_State','all_age_pop'],axis=1,inplace=True)\n",
    "    \n",
    "    nc = [c for c in dc.columns if c != 'obj_buildingType']\n",
    "    dn = dc[nc]\n",
    "    typ_h = pd.get_dummies(dc['obj_buildingType'],drop_first=True)\n",
    "    dh = pd.concat([dn,typ_h],axis=1)\n",
    "    \n",
    "    \n",
    "    dnlog = np.log(dn)\n",
    "    c  = []\n",
    "    for i in dnlog.columns:\n",
    "        i = i + '_log'\n",
    "        c.append(i)\n",
    "    dnlog.columns = c\n",
    "    if 'Price_Rent_Ratio_log' in dnlog.columns:\n",
    "        dnlog.drop(['Price_Rent_Ratio_log'],axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "    dr = dc.copy()\n",
    "    for i in dnlog.columns:\n",
    "        dr[i] = dnlog[i]\n",
    "    dr = dr.dropna()\n",
    "    if 'DG1_log' in dc.columns:\n",
    "        dr.drop(['DG1_log'],axis=1,inplace=True)\n",
    "    \n",
    "    return dc,dn,dh,dr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Creating new features from existing fearures so that it doesn't lead to problem of multi-coleniarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_house_macro = price_house.merge(macro)\n",
    "rent_house_macro = rent_house.merge(macro)\n",
    "\n",
    "price_whn = dp_whn.merge(macro)\n",
    "rent_whn = dr_whn.merge(macro)\n",
    "\n",
    "\n",
    "\n",
    "price_house_macro['price_m2'] = price_house_macro['obj_purchasePrice']/price_house_macro['obj_livingSpace']\n",
    "price_house_macro['a_r'] = price_house_macro['obj_lotArea']/price_house_macro['obj_livingSpace']\n",
    "\n",
    "rent_house_macro['anual_rent_m2'] = 12*rent_house_macro['obj_totalRent']/rent_house_macro['obj_livingSpace']\n",
    "rent_house_macro['a_r'] = rent_house_macro['obj_lotArea']/rent_house_macro['obj_livingSpace']\n",
    "\n",
    "price_house_macro = price_house_macro[price_house_macro['obj_buildingType'] != 'no_information']\n",
    "rent_house_macro = rent_house_macro[rent_house_macro['obj_buildingType'] != 'no_information']\n",
    "\n",
    "\n",
    "price_whn['area_per_room'] = price_whn['obj_livingSpace']/price_whn['obj_noRooms']\n",
    "rent_whn['area_per_room'] = rent_whn['obj_livingSpace']/rent_whn['obj_noRooms']\n",
    "\n",
    "price_hou = price_house_macro.drop(['obj_ExclusiveExpose', 'obj_cellar', 'obj_purchasePrice', 'obj_barrierFree', 'obj_regio3', 'obj_regio2', 'a_r'],axis=1)\n",
    "rent_hou = rent_house_macro.drop(['obj_ExclusiveExpose', 'obj_cellar', 'obj_totalRent', 'obj_barrierFree', 'obj_regio3', 'obj_regio2', 'a_r'],axis=1)\n",
    "\n",
    "price_hou.columns = ['obj_regio1', 'obj_lotArea', 'yearConstructed', 'obj_livingSpace',\n",
    "       'zip', 'obj_condition', 'obj_interiorQual', 'obj_noRooms',\n",
    "       'obj_buildingType', 'DG', 'A_total_of_1_room', '1_or_2_rooms',\n",
    "       '3_rooms', '4_rooms', '5_or_more_rooms',\n",
    "       'Total_population_of_age_18_25', 'Total_populatio__of_age_25_30',\n",
    "       'Total_population__of_age_30_40', 'Total_population_of_age_40_50',\n",
    "       'Deutsche', 'Foreigners', 'Wage_and_income_taxpayers',\n",
    "       'Total_amount_of_income', 'Wage_and_income_tax',\n",
    "       'gross_domestic_product',\n",
    "       'Gross_domestic_product_per_person_in_employment',\n",
    "       'Gross_domestic_product_per_inhabitant', 'amenity', 'bus',\n",
    "       'public_transport', 'cafe', 'doctors', 'fast_food', 'hospital',\n",
    "       'restaurant', 'zip_population', 'Population_State',\n",
    "       'GDP_Per_Capita_state', 'Area_State', 'log_price', 'Lat_reg2',\n",
    "       'Lng_reg2', 'Lat', 'Lng', 'lat_c', 'lng_c', 'price_m2']\n",
    "\n",
    "rent_hou.columns = ['obj_regio1', 'obj_lotArea', 'yearConstructed', 'obj_livingSpace',\n",
    "       'zip', 'obj_condition', 'obj_interiorQual', 'obj_noRooms',\n",
    "       'obj_buildingType', 'DG', 'A_total_of_1_room', '1_or_2_rooms',\n",
    "       '3_rooms', '4_rooms', '5_or_more_rooms',\n",
    "       'Total_population_of_age_18_25', 'Total_populatio__of_age_25_30',\n",
    "       'Total_population__of_age_30_40', 'Total_population_of_age_40_50',\n",
    "       'Deutsche', 'Foreigners', 'Wage_and_income_taxpayers',\n",
    "       'Total_amount_of_income', 'Wage_and_income_tax',\n",
    "       'gross_domestic_product',\n",
    "       'Gross_domestic_product_per_person_in_employment',\n",
    "       'Gross_domestic_product_per_inhabitant', 'amenity', 'bus',\n",
    "       'public_transport', 'cafe', 'doctors', 'fast_food', 'hospital',\n",
    "       'restaurant', 'zip_population', 'Population_State',\n",
    "       'GDP_Per_Capita_state', 'Area_State', 'log_price', 'Lat_reg2',\n",
    "       'Lng_reg2', 'Lat', 'Lng', 'lat_c', 'lng_c', 'anual_rent_m2']\n",
    "\n",
    "price_hou['area_per_room'] = price_hou['obj_livingSpace']/price_hou['obj_noRooms']\n",
    "rent_hou['area_per_room'] = rent_hou['obj_livingSpace']/rent_hou['obj_noRooms']\n",
    "\n",
    "price_hou.drop('obj_lotArea',axis=1,inplace=True)\n",
    "rent_hou.drop('obj_lotArea',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the all price data and all rental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_frames = [rent_whn,rent_hou]\n",
    "price_frames = [price_whn,price_hou]\n",
    "\n",
    "rent = rent_whn.append(rent_hou,ignore_index=True)\n",
    "price = price_whn.append(price_hou,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers using KNN method - automated method using Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.anomaly import *\n",
    "exp_ano = setup(rent)\n",
    "\n",
    "knn = create_model('knn')\n",
    "knn_df = assign_model(knn)\n",
    "rent = knn_df[knn_df['Label'] == 0].drop(['Label','Score'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.anomaly import *\n",
    "exp_ano = setup(price)\n",
    "\n",
    "knn = create_model('knn')\n",
    "knn_df = assign_model(knn)\n",
    "price = knn_df[knn_df['Label'] == 0].drop(['Label','Score'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions to create Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_merge_median_only(by,rent_house_macro,price_house_macro):\n",
    "    d1 = pd.pivot_table(rent_house_macro, index=[by,'obj_buildingType'],aggfunc=np.median).reset_index(level=['obj_buildingType'])\n",
    "    d2 = pd.pivot_table(price_house_macro, index=[by,'obj_buildingType'],aggfunc=np.median).reset_index(level=['obj_buildingType'])\n",
    "    d3 = d1.merge(d2,on=by)\n",
    "    d3 = d3[d3['obj_buildingType_x'] == d3['obj_buildingType_y']]\n",
    "    \n",
    "    d4 = d3.copy()\n",
    "    \n",
    "    d4['delta_year'] = d4['yearConstructed_x'] - d4['yearConstructed_y']\n",
    "    d4['delta_area_per_room'] = d4['area_per_room_x'] - d4['area_per_room_y']\n",
    "    d4['delta_livingspace'] = d4['obj_livingSpace_x'] - d4['obj_livingSpace_y']\n",
    "    d4['delta_norooms'] = d4['obj_noRooms_x'] - d4['obj_noRooms_y']\n",
    "    return d4\n",
    "\n",
    "def pivot_merge_median(by,rent_house_macro,price_house_macro):\n",
    "    d1 = pd.pivot_table(rent_house_macro, index=[by,'obj_buildingType'],aggfunc=np.median).reset_index(level=['obj_buildingType'])\n",
    "    rent_sup = pd.pivot_table(rent_house_macro, index=[by,'obj_buildingType'],aggfunc='count').reset_index(level=['obj_buildingType'])['yearConstructed']\n",
    "    d1['rent_supply'] = rent_sup\n",
    "    \n",
    "    d2 = pd.pivot_table(price_house_macro, index=[by,'obj_buildingType'],aggfunc=np.median).reset_index(level=['obj_buildingType'])\n",
    "    price_sup = pd.pivot_table(price_house_macro, index=[by,'obj_buildingType'],aggfunc='count').reset_index(level=['obj_buildingType'])['yearConstructed']\n",
    "    d2['price_sup'] = price_sup\n",
    "    \n",
    "    d3 = d1.merge(d2,on=by)\n",
    "    d3 = d3[d3['obj_buildingType_x'] == d3['obj_buildingType_y']]\n",
    "    \n",
    "    d4 = d3.copy()\n",
    "    \n",
    "    d4['delta_year'] = d4['yearConstructed_x'] - d4['yearConstructed_y']\n",
    "    d4['delta_area_per_room'] = d4['area_per_room_x'] - d4['area_per_room_y']\n",
    "    d4['delta_livingspace'] = d4['obj_livingSpace_x'] - d4['obj_livingSpace_y']\n",
    "    d4['delta_norooms'] = d4['obj_noRooms_x'] - d4['obj_noRooms_y']\n",
    "    return d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pivot_merge_median('zip',rent,price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using KNN to remove outliers - Pycaret Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.anomaly import *\n",
    "exp_ano = setup(d3)\n",
    "\n",
    "knn = create_model('knn')\n",
    "knn_df = assign_model(knn)\n",
    "d3 = knn_df[knn_df['Label'] == 0].drop(['Label','Score'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually removing some of the remaining outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d3[d3['delta_year'] < 200]\n",
    "d3 = d3[d3['delta_year'] > -200]\n",
    "\n",
    "d3 = d3[d3['delta_norooms'] > -10]\n",
    "d3 = d3[d3['delta_norooms'] < 10]\n",
    "\n",
    "\n",
    "d3.drop(['delta_year','delta_area_per_room','delta_livingspace','delta_norooms'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_refine(d4):\n",
    "    d3 = d4.copy()\n",
    "    ex = ['price_m2', 'anual_rent_m2']\n",
    "    cols = ['obj_buildingType', 'Area_State', 'DG', 'Deutsche', 'Foreigners', 'GDP_Per_Capita_state', 'Gross_domestic_product_per_inhabitant', 'Gross_domestic_product_per_person_in_employment', 'Lat', 'Lat_reg2', 'Lng', 'Lng_reg2', 'Population_State', 'Total_amount_of_income', 'Total_populatio__of_age_25_30', 'Total_population__of_age_30_40', 'Total_population_of_age_18_25', 'Total_population_of_age_40_50', 'Wage_and_income_tax', 'Wage_and_income_taxpayers', 'gross_domestic_product', 'public_transport', 'restaurant', 'zip_population']\n",
    "    #return d3\n",
    "    \n",
    "    d3['age'] = 2022 - .5*(d3['yearConstructed_x'] + d3['yearConstructed_y'])\n",
    "    d3['rooms'] = .5*(d3['obj_noRooms_x'] + d3['obj_noRooms_y'])\n",
    "    d3['livingSpace'] = .5*(d3['obj_livingSpace_x'] + d3['obj_livingSpace_y'])\n",
    "    d3['avg_space_per_room'] = .5*(d3['area_per_room_x'] + d3['area_per_room_y'])\n",
    "    \n",
    "    d3.drop(['log_price_x','log_price_y','yearConstructed_x','yearConstructed_y','obj_noRooms_x','obj_noRooms_y','obj_livingSpace_x','obj_livingSpace_y','area_per_room_x','area_per_room_y'],axis=1,inplace=True)\n",
    "    \n",
    "    c = []\n",
    "    for i in d3.columns:\n",
    "        if '_x' in i:\n",
    "            i = i.replace('_x','')\n",
    "        c.append(i)\n",
    "    d3.columns = c\n",
    "    \n",
    "    for i in d3.columns:\n",
    "        if '_y' in i:\n",
    "            d3.drop(i,axis=1,inplace=True)\n",
    "            \n",
    "    de = ['log_price','1_or_2_rooms', '3_rooms', '4_rooms',\n",
    "       '5_or_more_rooms', 'A_total_of_1_room',\n",
    "        'obj_totalRent','obj_purchasePrice','a_r']\n",
    "    \n",
    "    for i in de:\n",
    "        if i in d3.columns:\n",
    "            d3.drop(i,axis=1,inplace=True)\n",
    "\n",
    "            \n",
    "    d3['Price_Rent_ratio'] = d3['price_m2']/d3['anual_rent_m2']\n",
    "    d3.drop(['price_m2','anual_rent_m2'],axis=1,inplace=True)\n",
    "    \n",
    "    #print(d3.columns)\n",
    "    \n",
    "    dc = d3.copy()\n",
    "\n",
    "\n",
    "\n",
    "    change_col = ['Total_populatio__of_age_25_30', 'Total_population__of_age_30_40',\n",
    "           'Total_population_of_age_18_25', 'Total_population_of_age_40_50']\n",
    "    \n",
    "    dc['population_18_30'] = dc['Total_populatio__of_age_25_30'] + dc['Total_population_of_age_18_25']\n",
    "    dc['population_30_50'] = dc['Total_population__of_age_30_40'] + dc['Total_population_of_age_40_50']\n",
    "    \n",
    "\n",
    "    dc.drop(change_col,axis=1,inplace=True)\n",
    "\n",
    "    dc['population_density_state'] = dc['Population_State']/dc['Area_State']\n",
    "    dc.drop(['Area_State'],axis=1,inplace=True)\n",
    "    \n",
    "    dc['North_of_city'] = 111*(dc['Lat']-dc['lat_c'])\n",
    "    dc['East_of_city'] = 111*(dc['Lng']-dc['lng_c'])\n",
    "    \n",
    "    dc.drop(['DG','amenity','lat_c','lng_c','Lat_reg2','Lng_reg2'],axis=1,inplace=True)\n",
    "    dc = dc.fillna(dc.mean())\n",
    "    \n",
    "    #for i in ['cafe', 'doctors', 'fast_food', 'public_transport', 'restaurant']:\n",
    "        #dc[i] = dc[i] + 1\n",
    "    \n",
    "    dc['dist_aprox'] = np.sqrt(np.square(dc['North_of_city']) + np.square(dc['East_of_city']))\n",
    "    \n",
    "    nc = [c for c in dc.columns if c not in ['obj_buildingType','North_of_city','East_of_city']]\n",
    "    dn = dc[nc]\n",
    "    typ_h = pd.get_dummies(dc['obj_buildingType'],drop_first=True)\n",
    "    dh = pd.concat([dn,typ_h],axis=1)\n",
    "    \n",
    "    \n",
    "    dnlog = np.log(dn)\n",
    "    c  = []\n",
    "    for i in dnlog.columns:\n",
    "        i = i + '_log'\n",
    "        c.append(i)\n",
    "    dnlog.columns = c\n",
    "    if 'Price_Rent_Ratio_log' in dnlog.columns:\n",
    "        dnlog.drop(['Price_Rent_Ratio_log'],axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "    dr = dc.copy()\n",
    "    for i in dnlog.columns:\n",
    "        dr[i] = dnlog[i]\n",
    "    dr = dr.dropna()\n",
    "    if 'DG1_log' in dc.columns:\n",
    "        dr.drop(['DG1_log'],axis=1,inplace=True)\n",
    "    \n",
    "    dl = col_to_log(dn,'float64',[''],'Price_Rent_ratio')\n",
    "    dlog = dl.merge(dc)\n",
    "    \n",
    "    '''\n",
    "    r2,cl = forward_prop(dl,'Price_Rent_ratio','float64',[''])\n",
    "    cl.append('Price_Rent_ratio')\n",
    "    \n",
    "    md = dlog[cl]\n",
    "    dk = dc[['obj_buildingType','North_of_city','East_of_city']]\n",
    "    \n",
    "    md['in'] = list(range(len(md)))\n",
    "    dk['in'] = list(range(len(dk)))\n",
    "    \n",
    "\n",
    "\n",
    "    dfwd = dk.merge(md,on='in')\n",
    "\n",
    "    dfwd.drop('in',axis=1,inplace=True)\n",
    "    '''\n",
    "\n",
    "        \n",
    "    return dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding More Macro information about the distance to nearest city and exporting to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pivot_refine(d3)\n",
    "\n",
    "dcc = dc.copy()\n",
    "\n",
    "for i in range(len(city)):\n",
    "    latv = 'lat_city'+str(i)\n",
    "    lngv = 'lng_city'+str(i)\n",
    "    dcc[latv] = city['lat_c'].iloc[i]\n",
    "    dcc[lngv] = city['lng_c'].iloc[i]\n",
    "    \n",
    "for j in range(len(city)):\n",
    "    d0 = []\n",
    "    la0 = city['lat_c'].iloc[j]\n",
    "    ln0 = city['lng_c'].iloc[j]\n",
    "    for i in range(len(dcc)):\n",
    "        lat2 = dcc['Lat'].iloc[i]\n",
    "        lng2 = dcc['Lng'].iloc[i]\n",
    "        d_city = dist(la0,ln0,lat2,lng2)\n",
    "        d0.append(d_city)\n",
    "    print(len(d0),len(dcc))\n",
    "    var = 'dist_' + str(j)\n",
    "    dcc[var] = d0\n",
    "\n",
    "North = []\n",
    "East = []\n",
    "for i in range(len(dcc)):\n",
    "    lt = dcc['Lat'].iloc[i]\n",
    "    ln = dcc['Lng'].iloc[i]\n",
    "    j = dcc['nearest_city_id'].iloc[i]\n",
    "    ltc = dcc['lat_city'+str(j)].iloc[i]\n",
    "    lnc = dcc['lng_city'+str(j)].iloc[i]\n",
    "    north = 111*(lt-ltc)\n",
    "    east = 111*(ln-lnc)\n",
    "    North.append(north)\n",
    "    East.append(east)\n",
    "dcc['North_of_major_city'] = North\n",
    "dcc['East_of_major_city'] = East\n",
    "\n",
    "\n",
    "dcc['nearest_city_id'] = dcc[cdist].idxmin(axis=1)\n",
    "dcc['nearest_city_id'] = dcc['nearest_city_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "cdist = []\n",
    "for i in range(16):\n",
    "    cdist.append('dist_'+str(i))\n",
    "\n",
    "min_dist_city = dcc[cdist].min(axis=1)\n",
    "dcc['nearest_major_city'] = min_dist_city\n",
    "\n",
    "dc['nearest_major_city'] = dcc['nearest_major_city']\n",
    "dc['North_of_major_city'] = dcc['North_of_major_city']\n",
    "dc['East_of_major_city'] = dcc['East_of_major_city']\n",
    "\n",
    "dc.drop(['dist_aprox','North_of_city',\n",
    "       'East_of_city'],axis=1,inplace=True)\n",
    "\n",
    "dc.to_csv('all_median_pr_merged_with_no_percent_for_elasticity.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
